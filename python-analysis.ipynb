{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Talkspace is an online, text-based therapy service. I have been using the service for about a year, and over that time, I've noticed it's sometimes hard for me to stay engaged. There have been long stretches of times where I have not responded to my therapist. [Patient engagement is critical to developing a therapeutic relationship](http://dx.doi.org/10.3389/fpsyg.2015.02013), so if I can increase my own responsiveness, I might increase the efficacy of my therapy.\n",
    "\n",
    "Unfortunately, Talkspace hasn't provided me with any kind of patient engagemnt questionaire (such as the [Patient Activation Measure](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1361049/)), so I don't have any existing historical measures of enagement. The metrics most accessible to me that I might want to optimize for are my response time and how many words I write to my therapist per day I take to respond, i.e.\n",
    "\n",
    "$$\n",
    "\\frac{\\textrm{word count of message}}{\\textrm{time it takes me to write the message}}\n",
    "$$\n",
    "\n",
    "These quantities will obviously vary, and my suspicion is that some of that variability can be explained. If I can find a statistically significant factor that partly explains the variability, I'll have a lever to pull to improve my therapeutic relationship and my own mental health."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the data (you can do this too!)\n",
    "Prior to this analysis, I wrote an [tool](https://github.com/vaughn-johnson/talkspace-scraper) to allow anyone to scrape his or her Talkspace message history. If you are familiar with Javascript and NPM, you can use the tool to save that data to a database or export it directly as a file.\n",
    "\n",
    "This is obviously _highly sensitive data_. You should __never__ give your Talkspace username and password to a third party you do not trust. If you choose to use this tool, please exercise caution by [reviewing the code first](https://github.com/vaughn-johnson/talkspace-scraper). I appreciate that this stipulation might make the tool less accessible, but I strongly discourage anyone from blindly using this tool without being sure that his or her username and password will be secure.\n",
    "\n",
    "With that being said, here I am importing my own data from my own database. If you're unfamiliar with `dotenv`, I would _highly_ encourae reading about it [here](https://pypi.org/project/python-dotenv/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from pymongo import MongoClient\n",
    "from dotenv import load_dotenv\n",
    "from functools import reduce\n",
    "import collections\n",
    "import requests\n",
    "import json\n",
    "from inflection import humanize\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.graphics as sm_graphics\n",
    "import statsmodels.formula.api as sm_formula\n",
    "from patsy import dmatrices\n",
    "import textstat\n",
    "import re\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "MONGO_CONNECTION_STRING = os.getenv('MONGO_CONNECTION_STRING')\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "PATIENT_NAME = 'Vaughn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Other message types include automated messages from Talkspace\n",
    "RELEVANT_MESSAGE_TYPES = [1]\n",
    "\n",
    "messages = pd.DataFrame([\n",
    "    *MongoClient(os.getenv('MONGO_CONNECTION_STRING')).talkspace.messages.find(\n",
    "        { 'message_type': { '$in': RELEVANT_MESSAGE_TYPES } }\n",
    "    )\n",
    "])\n",
    "\n",
    "messages.created_at = messages.created_at.apply(pd.to_datetime)\n",
    "\n",
    "# I tend to quote my therapist and then delimit the beginning of my own\n",
    "# words using a \">\"\n",
    "#\n",
    "# Dallas tends to begin his messages with \"Vaughn,\"\n",
    "# and ends them with \"Respectfully, Dallas\"\n",
    "#\n",
    "# Dallas: Vaughn,\n",
    "#         lorem ipsum lakdfj\n",
    "#         Respectfully, Dallas\n",
    "#\n",
    "# Vaughn: \"lorem ipsum lakdfj\"\n",
    "#         > abalksjfd\n",
    "#\n",
    "ARROW_DELIMITER = re.compile('[^-]> ')\n",
    "EXTRATA = re.compile('(Vaughn,\\n*|Respectfully,\\n\\nDallas)')\n",
    "REPEAT_NEWLINES = re.compile('\\n\\n+')\n",
    "\n",
    "def extract_my_words(msg):\n",
    "    if not ARROW_DELIMITER.match(msg):\n",
    "        return msg\n",
    "\n",
    "    return ''.join(re.split(ARROW_DELIMITER, msg))[1:]\n",
    "\n",
    "def process_message(msg):\n",
    "    msg = extract_my_words(msg)\n",
    "    msg = re.sub(EXTRATA, '', msg)\n",
    "    msg = re.sub(REPEAT_NEWLINES, '\\n', msg)\n",
    "    return msg\n",
    "                   \n",
    "messages.message = messages.message.apply(process_message)\n",
    "\n",
    "# This is critical for rest of the analysis\n",
    "messages.sort_values('created_at', axis='rows', inplace=True)\n",
    "\n",
    "# This associates consecutive messages (in time) from the same person\n",
    "message_block_index = messages.user_id.ne(messages.user_id.shift()).cumsum()\n",
    "\n",
    "message_blocks = messages.groupby(message_block_index).agg({\n",
    "    'message': lambda l: '\\n'.join(l),\n",
    "    'created_at': min,\n",
    "    'display_name': 'first'\n",
    "})\n",
    "\n",
    "message_blocks['message_length'] = message_blocks.message.apply(len)\n",
    "message_blocks['question_count'] = message_blocks.message.apply(lambda x: len(re.findall(r'\\?', x)))\n",
    "message_blocks['word_count'] = message_blocks.message.apply(lambda x: len(re.findall(r'\\s', x)) + 1)\n",
    "message_blocks['readability'] = message_blocks.message.apply(textstat.flesch_reading_ease)\n",
    "\n",
    "message_blocks = pd.concat([message_blocks, message_blocks.shift().add_prefix('prev_')], axis='columns')\n",
    "\n",
    "# There are the quantities I'm interested in improving\n",
    "message_blocks['response_time'] = (message_blocks.created_at - message_blocks.prev_created_at) / pd.Timedelta(days=1)\n",
    "message_blocks['words_per_day'] = message_blocks['word_count'] / message_blocks['response_time']\n",
    "\n",
    "# Unfortunately, any null value will interfere with the model we fit below\n",
    "message_blocks.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE CODE ABOVE ACCESSES MY PRIVATE DATABASE. IF YOU DON'T\n",
    "# ACCESS TO THAT DATABASE, IT WON'T WORK. IN ORDER TO CONTINUE\n",
    "# YOU CAN USE THIS PUBLIC API WHICH RETURNS THE RESULTS OF THE\n",
    "# BLOCK ABOVE\n",
    "\n",
    "PUBLIC_API = 'https://us-central1-talkspace-293821.cloudfunctions.net/talkspace-public-api?format=json'\n",
    "\n",
    "message_blocks = pd.DataFrame(requests.get(PUBLIC_API).json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can pay no mind to the next block. It is exclusively used to make the plots appear nicer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Just for aesthetics\n",
    "class defaultdict(collections.defaultdict):\n",
    "    def __missing__(self, key):\n",
    "        if self.default_factory is None:\n",
    "            raise KeyError(key)\n",
    "        else:\n",
    "            default_value = self[key] = self.default_factory(key)\n",
    "            return default_value\n",
    "\n",
    "LABELS = defaultdict(humanize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration\n",
    "First, let's just look at the distribution of our features. For Clarity, my name is Vaughn, and my therapist's name is Dallas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(\n",
    "    message_blocks,\n",
    "    x='readability',\n",
    "    color='display_name',\n",
    "    facet_row='display_name',\n",
    "    labels=LABELS,\n",
    "    template='gridon',\n",
    "    title='Flesch Reading Ease Score of Messages'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(\n",
    "    message_blocks,\n",
    "    x='question_count',\n",
    "    color='display_name',\n",
    "    facet_row='display_name',\n",
    "    labels=LABELS,\n",
    "    template='gridon',\n",
    "    title=\"Number of questions asked (measured by appearence of \\\"?\\\")\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the quantities I'm trying to optimize for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(\n",
    "    message_blocks,\n",
    "    x='words_per_day',\n",
    "    color='display_name',\n",
    "    facet_row='display_name',\n",
    "    labels=LABELS,\n",
    "    template='gridon',\n",
    "    title=\"How many words typed per day it took to respond\",\n",
    "    nbins=1000,\n",
    "    range_x=[0, 3 * 10**3]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(\n",
    "    message_blocks,\n",
    "    x='response_time',\n",
    "    color='display_name',\n",
    "    facet_row='display_name',\n",
    "    labels=LABELS,\n",
    "    template='gridon',\n",
    "    title=\"Time to respond (in days)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(My therapist Dallas has much shorter response times than me)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some pair-wise plots of some of the featues I've extracted above that I think might explain how I interact with my therapist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "px.scatter_matrix(\n",
    "    message_blocks,\n",
    "    dimensions=[\n",
    "        \"words_per_day\",\n",
    "        \"response_time\",\n",
    "        \"prev_word_count\",\n",
    "        \"prev_question_count\",\n",
    "        \"prev_readability\"\n",
    "    ],\n",
    "    opacity=0.5,\n",
    "    color='display_name',\n",
    "    labels=LABELS,\n",
    "    title='Response data (Each point is a message, \"Prev\" means the previous message the point is responding to)',\n",
    "    template='gridon',\n",
    "    height=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing incredibly interesting jumps out here. We see some normally distributed varaibles plotted against uniformly distributed variables, which of course produces a bell shape. There isn't a clear relationship anywhere. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do the characteristics of my therapist's messages explain my responsiveness?\n",
    "\n",
    "Let's try to develop a model that tries to explain my responsiveness based on the messages I'm responding to. My hypothesis is that I might take longer to respond to more complex messages. Messages with a lower readability score, which are longer, or which ask me more question. I think if these were going to have have any affect, it would probably be linear, so fitting a generalized linear model with least squares seems appropriate.\n",
    "\n",
    "To be clear, this data is _not_ well suited for linear regression. The response variable I'm interested in does not appear to be normally distributed with any of its covariates, and the covariants aren't perfectly non-collinear. The observations are obviously not independent (though they don't seem to show any obvious autocorrelation). However, I think it's ok to give it a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is just setting up for some tests below\n",
    "features = [\n",
    "    'prev_word_count',\n",
    "    'prev_question_count',\n",
    "    'prev_readability',\n",
    "]\n",
    "\n",
    "my_messages = message_blocks[message_blocks.display_name == PATIENT_NAME]\n",
    "\n",
    "\n",
    "response_time, X = dmatrices(f\"response_time ~ {' + '.join(features)}\",\n",
    "                 data=my_messages,\n",
    "                 return_type='dataframe')\n",
    "\n",
    "message_length, X = dmatrices(f\"message_length ~ {' + '.join(features)}\",\n",
    "                 data=my_messages,\n",
    "                 return_type='dataframe')\n",
    "\n",
    "words_per_day, X = dmatrices(f\"words_per_day ~ {' + '.join(features)}\",\n",
    "                 data=my_messages,\n",
    "                 return_type='dataframe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Message Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.OLS(message_length, X).fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_length_predictions = sm.OLS(message_length, X).fit().predict(X)\n",
    "message_length_residuals = message_length['message_length'] - message_length_predictions\n",
    "\n",
    "px.scatter(\n",
    "    labels={ 'x': 'Message Length', 'y': 'Residual' },\n",
    "    x=message_length['message_length'],\n",
    "    y=message_length_residuals,\n",
    "    template='gridon'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.OLS(response_time, X).fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_time_predictions = sm.OLS(response_time, X).fit().predict(X)\n",
    "response_time_residuals = response_time['response_time'] - response_time_predictions\n",
    "\n",
    "px.scatter(\n",
    "    labels={ 'x': 'Response Time', 'y': 'Residual' },\n",
    "    x=response_time['response_time'],\n",
    "    y=response_time_residuals,\n",
    "    template='gridon'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words per day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.OLS(words_per_day, X).fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_per_day_predictions = sm.OLS(words_per_day, X).fit().predict(X)\n",
    "words_per_day_residuals = words_per_day['words_per_day'] - words_per_day_predictions\n",
    "\n",
    "px.scatter(\n",
    "    labels={ 'x': 'Response length per time to respond', 'y': 'Residual' },\n",
    "    x=words_per_day['words_per_day'],\n",
    "    y=words_per_day_residuals,\n",
    "    template='gridon'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "There are two interacting conclusions from this\n",
    "1. Nothing that I looked at today from my therapists messages makes me send more messages per unit time\n",
    "2. The longer my therapist's messages are, the longer it takes me to respond\n",
    "\n",
    "What this really means is that there is some _compensatory_ effect between how long it takes me to respond to my therapist, and how much I write back, and that compensatory effect washes out any measurable effect from the length of the message I'm responding to. If my therapist sends me a long message, I will generally take longer, but respond with a longer message. This actually shows up as a slight correlation between my response time and word count (`r = 0.313`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_messages['response_time'].corr(my_messages['word_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(my_messages, x='response_time', y='word_count', trendline='ols', template='gridon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions and next steps\n",
    "\n",
    "I have a new rule of thumb for my therapist. For every additional paragraph you send me, you can expect an additional 18 hours or so for me to respond. However, when I finally do respond, my response will generally be slightly longer.\n",
    "\n",
    "## Next steps\n",
    "I would like to build out a language model of these messages. Sometimes my therapist characterizes some of the things I send to him as good or bad, and it would be fabulous to build a model that could predict that judgement on arbitrary text."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:talkspace-python] *",
   "language": "python",
   "name": "conda-env-talkspace-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
